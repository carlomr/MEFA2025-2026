{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee77bc74",
   "metadata": {},
   "source": [
    "# Convergence of the error\n",
    "In this notebook, we test the theoretical orders of convergence for the Poisson problem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\Delta u &= f \\quad\\text{in }\\Omega\\\\\n",
    "u &= g_D \\quad\\text{on }\\Gamma_D\\\\\n",
    "\\nabla u\\cdot n &= g_N \\quad\\text{on }\\Gamma_N.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Given a mesh with characteristic element size $h$ and a FE space of piecewise polynomial functions of degree $p$, we expect that the error between the FE solution $u_h$ and the exact one $u$ behaves as follows: there exists a constant $C>0$ such that\n",
    "$$\n",
    "\\left(\\int_\\Omega (u-u_h)^2 \\right)^{1/2}\\leq C h^{p+1}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\left(\\int_\\Omega (\\nabla u-\\nabla u_h)^2\\right)^{1/2} \\leq C h^{p}.\n",
    "$$\n",
    "The two bounds above are true  *provided that the function $u$ has at least $p+1$ square-integrable derivatives* (i.e., that the function is sufficiently regular), *otherwise the order of convergence is reduced*. \n",
    "\n",
    "The first is called the $L^2$ norm of the error, while the second one is the $H^1_0$ (or energy) norm.\n",
    "\n",
    "We consider the case where the exact solution is not known: look at the notebook where the order of convergence is computed for manifactured solutions before reading this one. \n",
    "\n",
    "Here, we set $f=1$, $\\Gamma_D = \\partial \\Omega$, and $\\Omega = (0,1)^2$. We compute the solution on a FE space with many degrees of freedom and we use that solution as our \"exact\" (reference) solution, computing different the convergence of solutions computed with less degrees of freedom against that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "from petsc4py.PETSc import ScalarType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c091ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ufl\n",
    "from dolfinx import fem, io, mesh, plot \n",
    "from dolfinx.fem import Expression, Function, form, assemble_scalar\n",
    "from dolfinx.fem.petsc import LinearProblem\n",
    "from ufl import ds, dx, grad, inner\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99322f1b",
   "metadata": {},
   "source": [
    "After the imports, we create a function that solves the Poisson problem, as in the other notebooks. It takes as an input the number of refinements per edge of the domain and the polynomial degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaff8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(n_el, p):\n",
    "    msh = mesh.create_rectangle(\n",
    "        comm=MPI.COMM_WORLD,\n",
    "        points=((0., 0), (1.0, 1.0)),\n",
    "        n=[n_el]*2,\n",
    "        cell_type=mesh.CellType.triangle,\n",
    "    )\n",
    "\n",
    "    def marker_fun(x):\n",
    "        left_edge = np.isclose(x[0], 0) \n",
    "        right_edge = np.isclose(x[0], 1) \n",
    "        top_edge = np.isclose(x[1], 1) \n",
    "        bottom_edge = np.isclose(x[1], 0)\n",
    "        return left_edge | right_edge | top_edge | bottom_edge\n",
    "\n",
    "    V = fem.functionspace(msh, (\"Lagrange\", p))\n",
    "    facets = mesh.locate_entities_boundary(\n",
    "        msh,\n",
    "        dim=(msh.topology.dim - 1),\n",
    "        marker=marker_fun,\n",
    "    )\n",
    "    dofs = fem.locate_dofs_topological(V=V, entity_dim=1, entities=facets)\n",
    "    bc = fem.dirichletbc(value=ScalarType(0), dofs=dofs, V=V)\n",
    "    u = ufl.TrialFunction(V)\n",
    "    v = ufl.TestFunction(V)\n",
    "    x = ufl.SpatialCoordinate(msh)\n",
    "    f = ScalarType(1)\n",
    "\n",
    "    a = inner(grad(u), grad(v)) * dx\n",
    "    L = inner(f, v) * dx \n",
    "    problem = LinearProblem(a, L, bcs=[bc], petsc_options={\"ksp_type\": \"preonly\", \"pc_type\": \"lu\"})\n",
    "    uh = problem.solve()\n",
    "\n",
    "    return uh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f1978",
   "metadata": {},
   "source": [
    "We compute the reference solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a016c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "uref = solve_problem(100, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa0732",
   "metadata": {},
   "source": [
    "We will need an interpolate function, that we will use to compare solutions computed on different grids and different FE spaces. This will be necessary to compare the $u_h$ we will compute with the $u_{\\mathrm{ref}}$ we have just computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298351c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(u1,\n",
    "                u2\n",
    "                ):\n",
    "    '''\n",
    "    Interpolate u1 to u2,\n",
    "    each comming from separate meshes\n",
    "    '''\n",
    "    rmesh = u2.function_space.mesh\n",
    "    rtopology = rmesh.topology\n",
    "    cmap = rtopology.index_map(rtopology.dim)\n",
    "    num_cells = cmap.size_local + cmap.num_ghosts\n",
    "    all_cells = np.arange(num_cells,dtype=np.int32)\n",
    "    nmmid_all = fem.create_interpolation_data(\n",
    "                                 u2.function_space,\n",
    "                                 u1.function_space,\n",
    "                                 all_cells,\n",
    "                                 padding=0,)\n",
    "    u2.interpolate_nonmatching(u1, cells=all_cells, interpolation_data=nmmid_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f687d49",
   "metadata": {},
   "source": [
    "We now write two functions that compute the two types of the error between $u_h$ and $u_{\\mathrm{ref}}$, that we have defined above. Compared to the other notebook, this functions are more complex because we have to project the solution $u_h$ on the space associated with $u_{\\mathrm{ref}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_error(uh, u_ref):\n",
    "    Vref = u_ref.function_space\n",
    "    uh_Vref = Function(Vref)\n",
    "    interpolate(uh, uh_Vref)\n",
    "\n",
    "    e_Vref = Function(Vref)\n",
    "    e_Vref.x.array[:] = uh_Vref.x.array - u_ref.x.array \n",
    "    error = form(e_Vref**2 * ufl.dx)\n",
    "\n",
    "    E = np.sqrt(assemble_scalar(error))\n",
    "    return E\n",
    "\n",
    "def H1_error(uh, u_ref):\n",
    "    Vref = u_ref.function_space\n",
    "    uh_Vref = Function(Vref)\n",
    "    interpolate(uh, uh_Vref)\n",
    "\n",
    "    e_Vref = Function(Vref)\n",
    "    e_Vref.x.array[:] = uh_Vref.x.array - u_ref.x.array \n",
    "\n",
    "    error = form(ufl.grad(e_Vref)**2 * ufl.dx)\n",
    "    E = np.sqrt(assemble_scalar(error))\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95dd22",
   "metadata": {},
   "source": [
    "We then compute the errors, for different grid sizes and for a fixed polynomial degree, here $p=1$. We consider different grids, starting with 4 elements per side of the domain, and adding 10 elements per side at each refinement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "Ns = range(4, 104, 10)\n",
    "L2err = np.zeros(len(Ns))\n",
    "H1err = np.zeros(len(Ns))\n",
    "h = np.zeros(len(Ns))\n",
    "\n",
    "for i, n in enumerate(Ns):\n",
    "    uh = solve_problem(n, p)\n",
    "\n",
    "    L2err[i] = L2_error(uh, uref)\n",
    "    H1err[i] = H1_error(uh, uref)\n",
    "    h[i] = 1./n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb7d9a",
   "metadata": {},
   "source": [
    "We now want to plot the behaviour of the errors with respect to the grid size. We start by doing this on a linear-linear plot. We invert the ordinate axis to show the error going down for smaller mesh sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(h, L2err, '-o')\n",
    "ax[0].invert_xaxis()\n",
    "ax[0].set_title('L2 error')\n",
    "ax[0].set_xlabel('$h$')\n",
    "ax[1].plot(h, H1err, '-o')\n",
    "ax[1].invert_xaxis()\n",
    "ax[1].set_title('H1 error')\n",
    "ax[1].set_xlabel('$h$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6897a12",
   "metadata": {},
   "source": [
    "In the plot above, it is hard to properly see the rate of convergence. We redo the plot in the $\\log-\\log$ scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9302db",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2)\n",
    "ax[0].loglog(h, L2err, '-o')\n",
    "ax[0].invert_xaxis()\n",
    "ax[0].set_title('L2 error')\n",
    "ax[0].set_xlabel('$h$')\n",
    "ax[1].loglog(h, H1err, '-o')\n",
    "ax[1].invert_xaxis()\n",
    "ax[1].set_title('H1 error')\n",
    "ax[1].set_xlabel('$h$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e46f75",
   "metadata": {},
   "source": [
    "We now exploit the fact that, if, for some $C, r>0$ independent of $h$\n",
    "$$ \n",
    "\\mathrm{e}(h) = C h^{r}\n",
    "$$\n",
    "then \n",
    "$$\n",
    "\\log(e(h)) = \\log(C) + r \\log(h),\n",
    "$$\n",
    "which implies that an error curve will show up as a line with slope $r$ in $\\log-\\log$ scale. We first estimate our computational orders of convergence: we don't know the value of $C$, but we have that, if the equation above for the error is satisfied, for two values $h_i, h_j$,\n",
    "$$\n",
    "\\log(e(h_i)) - \\log(e(h_j)) = r \\left[\\log(h_i) - \\log(h_j)\\right].\n",
    "$$\n",
    "We use this equation to estimate the value of $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2r_est = np.divide(np.diff(np.log(L2err)), np.diff(np.log(h)))\n",
    "H1r_est = np.divide(np.diff(np.log(H1err)), np.diff(np.log(h)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"L2 rate\": L2r_est, \n",
    "    \"H1 rate\": H1r_est\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd60fb2",
   "metadata": {},
   "source": [
    "We now plot the theoretical estimate as a straight line alongside the errors that we computed, in the $\\log-\\log$ scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2th = h**(p+1)\n",
    "H1th = h**p \n",
    "_, ax = plt.subplots(1, 2)\n",
    "ax[0].loglog(h, L2err, '-o')\n",
    "ax[0].loglog(h, L2th, '--', label=f\"$\\propto h^{p+1}$\")\n",
    "ax[0].invert_xaxis()\n",
    "ax[0].set_title('L2 error')\n",
    "ax[0].set_xlabel('$h$')\n",
    "ax[0].legend()\n",
    "ax[1].loglog(h, H1err, '-o')\n",
    "ax[1].loglog(h, H1th, '--', label=f\"$\\propto h^{p}$\")\n",
    "ax[1].invert_xaxis()\n",
    "ax[1].legend()\n",
    "ax[1].set_title('H1 error')\n",
    "ax[1].set_xlabel('$h$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9db00",
   "metadata": {},
   "source": [
    "We now repeat this whole procedure for a different polynomial degree $p=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92974ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 4\n",
    "Ns = range(4, 104, 10)\n",
    "L2err = np.zeros(len(Ns))\n",
    "H1err = np.zeros(len(Ns))\n",
    "h = np.zeros(len(Ns))\n",
    "\n",
    "for i, n in enumerate(Ns):\n",
    "    uh = solve_problem(n, p)\n",
    "\n",
    "    L2err[i] = L2_error(uh, uref)\n",
    "    H1err[i] = H1_error(uh, uref)\n",
    "    h[i] = 1./n\n",
    "\n",
    "L2th = h**(p+1)\n",
    "L2th2 = h**(3)\n",
    "H1th = h**p \n",
    "H1th2 = h**2 \n",
    "_, ax = plt.subplots(1, 2)\n",
    "ax[0].loglog(h, L2err, '-o')\n",
    "ax[0].loglog(h, L2th, '--', label=f\"$\\propto h^{p+1}$\")\n",
    "ax[0].loglog(h, L2th2, 'k--', label=f\"$\\propto h^{3}$\")\n",
    "ax[0].invert_xaxis()\n",
    "ax[0].set_title('L2 error')\n",
    "ax[0].set_xlabel('$h$')\n",
    "ax[0].legend()\n",
    "ax[1].loglog(h, H1err, '-o')\n",
    "ax[1].loglog(h, H1th, '--', label=f\"$\\propto h^{p}$\")\n",
    "ax[1].loglog(h, H1th2, 'k--', label=f\"$\\propto h^{2}$\")\n",
    "ax[1].invert_xaxis()\n",
    "ax[1].legend()\n",
    "ax[1].set_title('H1 error')\n",
    "ax[1].set_xlabel('$h$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "L2r_est = np.divide(np.diff(np.log(L2err)), np.diff(np.log(h)))\n",
    "H1r_est = np.divide(np.diff(np.log(H1err)), np.diff(np.log(h)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"L2 rate\": L2r_est, \n",
    "    \"H1 rate\": H1r_est\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8ea4f",
   "metadata": {},
   "source": [
    "We can observe that in this domain, we do not see the \"expected\" order of convergence. This is due to the fact that the domain has corners, and the exact solution does not have, therefore, the required regularity for that rate to be observed."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "MEFA-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
